{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction to Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Import necessary packages we need. \"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f2a820df4262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read stopword list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\" Simple text cleaning \"\"\"\n",
    "\n",
    "# Read stopword list\n",
    "with open('stopwords.txt') as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [w.strip() for w in stopwords]\n",
    "\n",
    "# Read dictionary of abbreviation as key and its orginal form as value\n",
    "with open('abbreviations.json') as f:\n",
    "    abbr_dict = json.load(f)\n",
    "\n",
    "\"\"\" Preprocessing utility for later use. \"\"\"\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    # Remove '\\n' and '\\'\n",
    "    text = text.lower().replace(\"\\\\n\", \"\").replace('\\\\', '')\n",
    "    # Replace abbreviation to its orginal form\n",
    "    for k, v in abbr_dict.items():\n",
    "        text = text.replace(k, v)\n",
    "    # Remove non-letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # Remove stop words and join the words back into one string separated by space\n",
    "    text = \" \".join([w for w in text.split() if not w in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Representation\n",
    "- Use one-hot vectors in the context of machine learning/deep learning.\n",
    "    - For example, our vocabulary is: {'dog, 'cat', 'bark', 'meow'}\n",
    "    - Then each word represents in one-hot will be:\n",
    "        - vec('dog') : [1, 0, 0, 0]\n",
    "        - vec('cat') : [0, 1, 0, 0]\n",
    "        - vec('bark'): [0, 0, 1, 0]\n",
    "        - vec('meow'): [0, 0, 0, 1]\n",
    "- Problems: \n",
    "    - Hard to measure word similarity.\n",
    "    - Cannot handle missing words.\n",
    "    - Hard to compute semantic relationships (e.g. \"I am good at NLP\" v.s. \"I am a NLP expert\")\n",
    "    - Inefficent to compute when vocabulary size is large.\n",
    "\n",
    "## Distributional Representation\n",
    "\n",
    "- Motivation: Preserve semantic information while having relatively low dimensionality for machine learning/deep learning. \n",
    "- Idea: Distributional hypothesis, \"Words that occur in similar contexts (with the same neighboring words), tend to have similar meanings.\".\n",
    "- Two main approaches: count-based model and predictive model.\n",
    "\n",
    "### Count-based Model\n",
    "- Idea: Compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, sometimes further maps these count-statistics down to a small, dense vector for each word.\n",
    "- Adavantage: Capture co-occurrence statistics of the corpus (\"global\" information).\n",
    "- Problem: Sparse, matrix is very large to compute (need a lot of memory).\n",
    "- Let's see the following simple example. We basically do the following things:\n",
    "    - Use `CountVectorizer` to build document-term matrix.\n",
    "    - Build word co-occurence matrix from document-term matrix.\n",
    "    - Each row/col of word co-occurence matrix is our word distributional representation.\n",
    "    - Measure the word similarity using cosine similarity.\n",
    "    - Visualize word vectors using SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'dog': 3, 'run': 6, 'cat': 2, 'sleep': 7, 'bark': 0, 'meows': 5, 'bird': 1, 'fly': 4}\n",
      "Unordered columns: ['dog', 'run', 'cat', 'sleep', 'bark', 'meows', 'bird', 'fly']\n",
      "Ordered columns: ['bark', 'bird', 'cat', 'dog', 'fly', 'meows', 'run', 'sleep']\n",
      "\n",
      "Document-Term Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bark</th>\n",
       "      <th>bird</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>fly</th>\n",
       "      <th>meows</th>\n",
       "      <th>run</th>\n",
       "      <th>sleep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the dog run.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the cat run.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the dog sleep.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the cat sleep.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the dog bark.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the cat meows.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the bird fly.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the bird sleep.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bark  bird  cat  dog  fly  meows  run  sleep\n",
       "the dog run.        0     0    0    1    0      0    1      0\n",
       "the cat run.        0     0    1    0    0      0    1      0\n",
       "the dog sleep.      0     0    0    1    0      0    0      1\n",
       "the cat sleep.      0     0    1    0    0      0    0      1\n",
       "the dog bark.       1     0    0    1    0      0    0      0\n",
       "the cat meows.      0     0    1    0    0      1    0      0\n",
       "the bird fly.       0     1    0    0    1      0    0      0\n",
       "the bird sleep.     0     1    0    0    0      0    0      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Co-occurence Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bark</th>\n",
       "      <th>bird</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>fly</th>\n",
       "      <th>meows</th>\n",
       "      <th>run</th>\n",
       "      <th>sleep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bark</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bird</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fly</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meows</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bark  bird  cat  dog  fly  meows  run  sleep\n",
       "bark      1     0    0    1    0      0    0      0\n",
       "bird      0     1    0    0    1      0    0      1\n",
       "cat       0     0    1    0    0      1    1      1\n",
       "dog       1     0    0    1    0      0    1      1\n",
       "fly       0     1    0    0    1      0    0      0\n",
       "meows     0     0    1    0    0      1    0      0\n",
       "run       0     0    1    1    0      0    1      0\n",
       "sleep     0     1    1    1    0      0    0      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Word-to-ID dictionary'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'bark': 0,\n",
       " 'bird': 1,\n",
       " 'cat': 2,\n",
       " 'dog': 3,\n",
       " 'fly': 4,\n",
       " 'meows': 5,\n",
       " 'run': 6,\n",
       " 'sleep': 7}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ID-to-Word dictionary'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{0: 'bark',\n",
       " 1: 'bird',\n",
       " 2: 'cat',\n",
       " 3: 'dog',\n",
       " 4: 'fly',\n",
       " 5: 'meows',\n",
       " 6: 'run',\n",
       " 7: 'sleep'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog vector: [1 0 0 1 0 0 1 1]\n",
      "cat vector: [0 0 1 0 0 1 1 1]\n",
      "bird vector: [0 1 0 0 1 0 0 1]\n",
      "bark vector: [1 0 0 1 0 0 0 0]\n",
      "\n",
      "dog v.s. cat: 0.5\n",
      "dog v.s. bird: 0.28867513459481287\n",
      "dog v.s. bark: 0.7071067811865475\n",
      "cat v.s. bark: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112bdaba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assume this is our data.\n",
    "documents = [\n",
    "    'the dog run.',\n",
    "    'the cat run.',\n",
    "    'the dog sleep.',\n",
    "    'the cat sleep.',\n",
    "    'the dog bark.',\n",
    "    'the cat meows.',\n",
    "    'the bird fly.',\n",
    "    'the bird sleep.'\n",
    "]\n",
    "\n",
    "# Contruct document-term matrix\n",
    "vectorizer = CountVectorizer(preprocessor=clean_text, analyzer='word', ngram_range=(1, 1))\n",
    "\n",
    "# Or... you can try `TfidfVectorizer`\n",
    "# vectorizer = TfidfVectorizer(preprocessor=clean_text, analyzer='word', ngram_range=(1, 1))\n",
    "\n",
    "document_term_matrix = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# Contruct word-occurence matrix from document-term matrix\n",
    "word_cooccurence_matrix = np.dot(document_term_matrix.T, document_term_matrix)\n",
    "np.fill_diagonal(word_cooccurence_matrix, 1)\n",
    "\n",
    "# Print mapping of term and feature index\n",
    "print('Vocabulary: {}'.format(vectorizer.vocabulary_))\n",
    "\n",
    "# We need to sort key by its index in order to visualize feature index in correct order.\n",
    "ordered_columns = []\n",
    "for col, index in vectorizer.vocabulary_.items():\n",
    "    ordered_columns.append((col, index))\n",
    "ordered_columns.sort(key=lambda x: x[1])\n",
    "ordered_columns = [col for col, index in ordered_columns]\n",
    "\n",
    "# If you iterate dictionary, it won't preserve the order\n",
    "unordered_columns = [col for col, index in vectorizer.vocabulary_.items()]\n",
    "\n",
    "print('Unordered columns: {}'.format(unordered_columns))\n",
    "print('Ordered columns: {}'.format(ordered_columns))\n",
    "\n",
    "# Visualize our document-term matrix\n",
    "print('\\nDocument-Term Matrix:')\n",
    "display(pd.DataFrame(document_term_matrix, index=documents, columns=ordered_columns))\n",
    "# Visualize our word-occurence matrix\n",
    "print('\\nWord Co-occurence Matrix:')\n",
    "display(pd.DataFrame(word_cooccurence_matrix, index=ordered_columns, columns=ordered_columns))\n",
    "\n",
    "word2id = vectorizer.vocabulary_\n",
    "id2word = {x[1]: x[0] for x in word2id.items()}\n",
    "\n",
    "display('Word-to-ID dictionary')\n",
    "display(word2id)\n",
    "display('ID-to-Word dictionary')\n",
    "display(id2word)\n",
    "\n",
    "vector_dog = word_cooccurence_matrix[word2id['dog']]\n",
    "vector_cat = word_cooccurence_matrix[word2id['cat']]\n",
    "vector_bird = word_cooccurence_matrix[word2id['bird']]\n",
    "vector_bark = word_cooccurence_matrix[word2id['bark']]\n",
    "\n",
    "print('dog vector: {}'.format(vector_dog))\n",
    "print('cat vector: {}'.format(vector_cat))\n",
    "print('bird vector: {}'.format(vector_bird))\n",
    "print('bark vector: {}\\n'.format(vector_bark))\n",
    "\n",
    "print('dog v.s. cat: {}'.format(1 - cosine(vector_dog, vector_cat)))\n",
    "print('dog v.s. bird: {}'.format(1 - cosine(vector_dog, vector_bird)))\n",
    "print('dog v.s. bark: {}'.format(1 - cosine(vector_dog, vector_bark)))\n",
    "print('cat v.s. bark: {}'.format(1 - cosine(vector_cat, vector_bark)))\n",
    "\n",
    "# Visualize word vectors using SVD.\n",
    "# Hint: Using TF-IDF will looks better.\n",
    "fig = plt.figure()\n",
    "U, sigma, Vh = np.linalg.svd(word_cooccurence_matrix)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis([-1, 1, -1, 1])\n",
    "for i in id2word:\n",
    "    ax.text(U[i, 0], U[i, 1], id2word[i], alpha=0.5, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model\n",
    "- Idea: Directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model).\n",
    "- Word2vec: A neural-based(shallow neural network) efficient model (Widely used nowadays)\n",
    "- Adavantage: Scanning context windows across the entire corpus needs less memory than count-based model.\n",
    "- Problem: Need more time to compute.\n",
    "\n",
    "## Word2vec\n",
    "- Two different objectives:\n",
    "    - *Continuous Bag of Words (CBOW)*: Predict target word from context words.\n",
    "    - *Skip-grams*: Predict context words from target word.\n",
    "- Two efficient training algorithms: *Negative Sampling* and *Hierarchical Softmax*.\n",
    "![CBOW-SG](img/cbow-sg.png)\n",
    "- We only introduce *Skip-grams* and *Negative Sampling* here.\n",
    "\n",
    "### Skip-gram\n",
    "- Objective: Given a target word, maximize the probability of the context word (Maximum likelihood estimation). For mathimatical convenience, it's equivalent to **minimizing negative log probability** since product becomes sum.\n",
    "- Maximizing the objective $J(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} p(w_{t+j} | w_t ; \\theta)$ turns to minimizing $J'(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log p(w_{t+j} | w_t ; \\theta)$.\n",
    "- $t$ denotes the index of center word, $m$ denotes window size.\n",
    "- With probability defined as: $p(o|c) = \\frac{exp(u_o^T v_c)}{\\sum_{w=1}^W exp(u_w^T v_c)}$, where $o$ denotes output word index, $c$ denotes center(target) word index and $W$ is number of vocabulary.\n",
    "- Dot product $u^T v = u \\cdot v$: Bigger if $u$ and $v$ is similar!\n",
    "- Softmax $\\frac{exp(z_i)}{\\sum_j exp(z_j)}$: $exp(z_i)$ yields positive numbers; dividing $\\sum_j exp(z_j)$ normalizes the output to probability distribution.\n",
    "- Compute gradients of $\\theta$ w.r.t $v_c$ and update $\\theta$ by sochastic gradient descent: $\\theta^{new} = \\theta^{old} - \\alpha\\frac{\\partial}{\\partial\\theta^{old}}J'(\\theta)$, where $\\alpha$ is *learning rate*.\n",
    "- Problem of softmax function: For each training step, we need to do the gigantic sum ($\\sum_{w=1}^W exp(u_w^T v_c)$ term in softmax), which is very computationally expensive.\n",
    "![SG](img/sg.png)\n",
    "\n",
    "### Negative Sampling (Simplified Version of Noise Contrastive Estimation; NCE)\n",
    "- Idea: Instead of doing giganic sum in softmax function, we turn our previous objective into **maximizing probability of true pair (center word paired with word in its context window)** and **minimizing noise pairs (center word paired with a random word)**\n",
    "- New objective is defined as: $\\log\\sigma(u_o^T v_c) + \\sum_{i=1}^k \\mathbb{E}_{j~P(w)}[log \\sigma(-u_j^T v_c)]$, where there are $k$ negative samples for each data sample. $k$ in the range 5–20 are useful for small training datasets, while for large datasets the $k$ can be as\n",
    "small as 2–5.\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ maps $x$ to 0~1, which mimics the measurement of probability.\n",
    "\n",
    "## More popular word/sentence embeddings to use:\n",
    "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "- [FastText by Facebook](https://github.com/facebookresearch/fastText)\n",
    "- [Paragraph2vec/Doc2vec (Extenstion of word2vec)](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "- [Skip-Thought Vectors](https://github.com/ryankiros/skip-thoughts)\n",
    "- [Sense2Vec](https://github.com/explosion/sense2vec)\n",
    "- https://github.com/Hironsan/awesome-embedding-models\n",
    "\n",
    "## Relative papers:\n",
    "- [Don’t count, predict! A systematic comparison ofcontext-counting vs. context-predicting semantic vectors](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf)\n",
    "- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "- [word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722v1.pdf)\n",
    "- [Neural Word Embedding as Implicit Matrix Factorization](https://arxiv.org/pdf/1402.3722v1.pdf)\n",
    "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- [Improving Distributional Similarity with Lessons Learned from Word Embeddings](http://www.aclweb.org/anthology/Q15-1016)\n",
    "- [Evaluation methods for unsupervised word embeddings](http://www.aclweb.org/anthology/D15-1036)\n",
    "\n",
    "## Reference:\n",
    "The above content is covered in more detail on [Standford Cs224d Deep learning for NLP](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aa82016031da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Clean text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# Prepare input format for training word2vec.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-aa82016031da>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Clean text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# Prepare input format for training word2vec.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Assume this is our data.\n",
    "documents = [\n",
    "    'the dog run.',\n",
    "    'the cat run.',\n",
    "    'the dog sleep.',\n",
    "    'the cat sleep.',\n",
    "    'the dog bark.',\n",
    "    'the cat meows.',\n",
    "    'the bird fly.',\n",
    "    'the bird sleep.'\n",
    "]\n",
    "\n",
    "# Clean text.\n",
    "documents = [clean_text(doc) for doc in documents]\n",
    "# Prepare input format for training word2vec.\n",
    "documents = [doc.split() for doc in documents]\n",
    "print('Input for word2vec:')\n",
    "display(documents)\n",
    "\n",
    "# Start to train word2vec.\n",
    "\"\"\"\n",
    "    @param `sg`: sg defines the training algorithm (skip-gram). By default (sg=0), CBOW is used. Otherwise (sg=1).\n",
    "    @param `size`: number of dimension of the feature vectors.\n",
    "    @param `window`: maximum distance between the current and predicted word within a sentence.\n",
    "    @param `min_count`: ignore all words with total frequency lower than this.\n",
    "    @param `negative`: Specifies how many “noise words” should be drawn (usually between 5-20). (Default=5).\n",
    "                       5–20 are useful for small training datasets, while for large datasets the k can be as\n",
    "                       small as 2–5\n",
    "    More detail on: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\"\"\"\n",
    "print('Training word2vec ...\\n')\n",
    "model_w2v = gensim.models.Word2Vec(documents, sg=1, size=2, window=3, min_count=1, negative=5)\n",
    "\n",
    "print('Attributes in word2vec model:')\n",
    "display(vars(model_w2v.wv))\n",
    "\n",
    "# Visualize word vectors.\n",
    "min_x = np.min(model_w2v.wv.syn0[:, 0]) - 0.1\n",
    "max_x = np.max(model_w2v.wv.syn0[:, 0]) + 0.1\n",
    "min_y = np.min(model_w2v.wv.syn0[:, 1]) - 0.1\n",
    "max_y = np.max(model_w2v.wv.syn0[:, 1]) + 0.1\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axis([min_x, max_x, min_y, max_y])\n",
    "for word in model_w2v.wv.index2word:\n",
    "    vec = model_w2v[word]\n",
    "    ax.text(vec[0], vec[1], word, alpha=0.5, fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word to \"dog\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cat', 0.9940736889839172),\n",
       " ('fly', 0.9890843629837036),\n",
       " ('bird', 0.5971224904060364),\n",
       " ('meows', 0.35978227853775024),\n",
       " ('run', 0.32364344596862793)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word to \"cat\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fly', 0.9992409944534302),\n",
       " ('dog', 0.9940736889839172),\n",
       " ('bird', 0.6807843446731567),\n",
       " ('sleep', 0.2819664776325226),\n",
       " ('meows', 0.2562209963798523)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word to \"bird\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sleep', 0.8947212100028992),\n",
       " ('fly', 0.708801805973053),\n",
       " ('cat', 0.6807844042778015),\n",
       " ('dog', 0.5971225500106812),\n",
       " ('meows', -0.533600926399231)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can use `most_similar()` to explore the top-5 similar words.\n",
    "# Cosine similarity has a range of [-1, 1]\n",
    "# You could interpret negative scores as \"opposite meaning\".\n",
    "print('Most similar word to \"dog\":')\n",
    "display(model_w2v.most_similar('dog', topn=5))\n",
    "\n",
    "print('Most similar word to \"cat\":')\n",
    "display(model_w2v.most_similar('cat', topn=5))\n",
    "\n",
    "print('Most similar word to \"bird\":')\n",
    "display(model_w2v.most_similar('bird', topn=5))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "7156fad3f0ae278ee111a7d1aaf63e77e30e4ef15f500a99c563bbdae96cd037"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
